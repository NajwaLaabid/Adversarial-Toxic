{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"experiments.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1ZgewQespVRWr2UsoFjWdUUxZOtOWhL7S","authorship_tag":"ABX9TyNqm5JtJ1dd4ZTRxhU+6DKH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"AayBzxrT4LbA","colab_type":"text"},"source":["General colab instantiations"]},{"cell_type":"code","metadata":{"id":"aZUABfqjC9_-","colab_type":"code","colab":{}},"source":["%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l3v6m9m5tnmm","colab_type":"code","colab":{}},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_jOKf32WzM73","colab":{}},"source":["import os\n","\n","path = 'drive/My Drive/TUM/adversarial-toxic' #add path of project folder in your G-drive\n","\n","os.chdir(path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vpcjcdu54UKE","colab_type":"text"},"source":["Code imports"]},{"cell_type":"code","metadata":{"id":"ng86TyV63SnM","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from torch.autograd import Variable\n","import argparse\n","import shutil\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, auc, f1_score\n","import copy\n","\n","# local imports\n","import model\n","import scoring\n","import transformer\n","import advloaddata\n","import dataloader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JS4owQPx4Y9m","colab_type":"text"},"source":["GPU related init."]},{"cell_type":"code","metadata":{"id":"8p-s64Nj3Wzl","colab_type":"code","colab":{}},"source":["# gpu related init\n","torch.manual_seed(7)\n","torch.cuda.manual_seed_all(7)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fFumDWHmHOZ2","colab_type":"text"},"source":["General variables\n"]},{"cell_type":"code","metadata":{"id":"NYUMEwDrHNdP","colab_type":"code","colab":{}},"source":["args = argparse.Namespace(\n","  data = 0,\n","  charlength = 1014,\n","  wordlength = 500,\n","  space = False,\n","  trans = False,\n","  backward = -1,\n","  batchsize = 128,\n","  maxnorm = 400,\n","  maxbatches = None,\n","  advsamplepath = None,\n","  externaldata = '',\n","  dictionarysize = 20000,\n","  lr = 0.0005,\n","  epochs = 10,\n","  power = 1\n",")\n","\n","DEBUG_L1 = True\n","DEBUG_L2 = True\n","\n","models = [\"bilstm\",\"simplernn\",\"wordcnn\"]\n","datatypes = [\"word\",\"word\",\"word\"]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sDrhmRWTZZHF","colab_type":"text"},"source":["Helper functions"]},{"cell_type":"code","metadata":{"id":"8xEI95AxZYU_","colab_type":"code","colab":{}},"source":["def debug(message, b):\n","  \"\"\"\n","    Used to print debugging messages if enabled.\n","\n","    Parameters: \n","    message: to print (can be any object printable by python)\n","    b: boolean to specify the level of debugging\n","  \"\"\"\n","  if b: print(message)\n","\n","def save_checkpoint(state, is_best, filename='checkpoint.dat'):\n","  \"\"\"\n","    Saves a trained model.\n","\n","    Parameters:\n","    state: a dictionary object containing state info on the model.\n","    is_best: boolean to make an extra copy of the best model.\n","    filename: the name of the file where the model is to be saved.\n","  \"\"\"\n","  torch.save(state, filename + '_checkpoint.dat')\n","  if is_best:\n","    shutil.copyfile(filename + '_checkpoint.dat', filename + \"_bestmodel.dat\")\n","\n","def plot_roc(fpr_l, tpr_l, auc_l):\n","  \"\"\"\n","    Plots multiple ROC curves.\n","\n","    Parameters:\n","    fpr_l: list of false positive rates (x values of curve)\n","    tpr_l: list of true positive rates (y values of curve)\n","    auc_l: list of area under the curve (auc) values for every curve drawn.\n","  \"\"\"\n","  colors = ['darkorange','green','magenta']\n","  model_name = [\"Bi-LSTM\",\"Simple RNN\",\"Simple CNN\"]\n","  \n","  plt.figure()\n","  lw = 2\n","  for i in range(len(auc_l)):\n","    plt.plot(fpr_l[i], tpr_l[i], color=colors[i], \n","      lw=lw, label='%s (area = %0.2f)' % (model_name[i],auc_l[i]))\n","\n","  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n","  plt.xlim([0.0, 1.0])\n","  plt.ylim([0.0, 1.05])\n","  plt.xlabel('False Positive Rate')\n","  plt.ylabel('True Positive Rate')\n","  plt.title('Receiver operating characteristic for models')\n","  plt.legend(loc=\"lower right\")\n","  plt.savefig(\"training_rocs.png\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RnfapBB-A0RC","colab_type":"text"},"source":["Loading data\n"]},{"cell_type":"code","metadata":{"id":"XCYXTGceAzNO","colab_type":"code","colab":{}},"source":["debug(\"Loading data..\", DEBUG_L1)\n","# data for word models\n","(train, adv, test, tokenizer, numclass, rawtrain, rawadv, rawtest) = advloaddata.loaddatawithtokenize(args.data, nb_words = args.dictionarysize, datalen = args.wordlength, withraw = True)\n","word_index = tokenizer.word_index\n","trainword_set = dataloader.Worddata(train, getidx = True, rawdata = rawtrain)\n","advword_set = dataloader.Worddata(adv, getidx = True, rawdata = rawadv)\n","testword_set = dataloader.Worddata(test, getidx = True, rawdata = rawtest)\n","trainword_loader = DataLoader(trainword_set, batch_size = args.batchsize, num_workers = 4, shuffle = True)\n","advword_loader = DataLoader(advword_set, batch_size = args.batchsize, num_workers = 4, shuffle = True)\n","testword_loader = DataLoader(testword_set, batch_size = args.batchsize, num_workers = 4)\n","maxlength =  args.wordlength\n","\n","# feedback on loading\n","debug(\"Data loaded.\", DEBUG_L1)\n","debug(\"Size of training set: %d\" % len(trainword_set.inputs), DEBUG_L1)\n","debug(\"Size of test set: %d\" % len(testword_set.inputs), DEBUG_L1)\n","debug(\"Size of adversarial set: %d\" % len(advword_set.inputs), DEBUG_L1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cq99wp48ujkF","colab_type":"code","colab":{}},"source":["# get basic statistics on dataset\n","train_labels = trainword_set.labels\n","test_labels = testword_set.labels\n","adv_labels = advword_set.labels\n","\n","debug(\"====== train ======\", DEBUG_L1)\n","debug(\"Total number of instances: %d\" % len(train_labels), DEBUG_L1)\n","debug(\"Total number of positive class instances: %d\" % len(train_labels[train_labels == 1]), DEBUG_L1)\n","debug(\"Total number of negative class instances: %d\" % len(train_labels[train_labels == 0]), DEBUG_L1)\n","\n","debug(\"====== test ======\", DEBUG_L1)\n","debug(\"Total number of instances: %d\" % len(test_labels), DEBUG_L1)\n","debug(\"Total number of positive class instances: %d\" % len(test_labels[test_labels == 1]), DEBUG_L1)\n","debug(\"Total number of negative class instances: %d\" % len(test_labels[test_labels == 0]), DEBUG_L1)\n","\n","debug(\"====== adv ======\", DEBUG_L1)\n","debug(\"Total number of instances: %d\" % len(adv_labels), DEBUG_L1)\n","debug(\"Total number of positive class instances: %d\" % len(adv_labels[adv_labels == 1]), DEBUG_L1)\n","debug(\"Total number of negative class instances: %d\" % len(adv_labels[adv_labels == 0]), DEBUG_L1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rrsHJ1e0RuWT","colab_type":"code","colab":{}},"source":["def get_model(model_name, numclass):\n","  \"\"\"\n","    Generates a model of a specified kind and number of output classes.\n","\n","    Returns:\n","    Generated model moved to CUDA device if possible/applicable.\n","\n","    Parameters:\n","    model_name: type of model to generate.\n","    numclass: number of output classes of the model\n","  \"\"\"\n","  if model_name == \"charcnn\":\n","      model_instance = model.CharCNN(classes = numclass)\n","  elif model_name == \"simplernn\":\n","      model_instance = model.smallRNN(classes = numclass)\n","  elif model_name == \"bilstm\":\n","      model_instance = model.smallRNN(classes = numclass, bidirection = True)\n","  elif model_name == \"smallcharrnn\":\n","      model_instance = model.smallcharRNN(classes = numclass)\n","  elif model_name == \"wordcnn\":\n","      model_instance = model.WordCNN(classes = numclass)\n","\n","  model_instance = model_instance.to(device)\n","  debug(model_instance, DEBUG_L1)\n","\n","  return model_instance\n","\n","def train_model(model_name, model, train_loader, test_loader):\n","  \"\"\"\n","    Trains a given model on train_loader and tests it on test_loader in every epoch.\n","\n","    Returns:\n","    the predictions of the best model achieved.\n","\n","    Parameters:\n","    model_name: name/type of model being trained (used to save the model state to a file)\n","    model: the model to train.\n","    train_loader: pytorch data loader for trainset.\n","    test_loader: pytorch data loader for testset.\n","  \"\"\"\n","  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","  bestacc = 0\n","  for epoch in range(args.epochs+1):\n","    debug('Start epoch %d' % epoch, DEBUG_L1)\n","    model.train()\n","    for dataid, data in enumerate(train_loader):\n","      try:\n","        inputs, target, idx, raw = data\n","      except:\n","        inputs, target = data\n","\n","      inputs, target = Variable(inputs),  Variable(target)\n","      inputs, target = inputs.to(device), target.to(device)\n","      output = model(inputs)\n","\n","      loss = F.nll_loss(output, target)\n","\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","    \n","    correct = .0\n","    total_loss = 0\n","    model.eval()\n","    all_pred = []\n","    for dataid, data in enumerate(test_loader):\n","        inputs, target, idx, raw = data\n","        inputs, target = inputs.to(device), target.to(device)\n","        output = model(inputs)\n","        loss = F.nll_loss(output, target)\n","        total_loss += loss.item()\n","        pred = output.data.max(1, keepdim=True)[1]\n","        all_pred.append(pred.cpu())\n","        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n","\n","    all_pred = torch.cat(all_pred[:-1], dim=0)\n","    all_pred = torch.cat((all_pred,pred.cpu()),dim=0)\n","\n","    acc = correct/len(test_loader.dataset)\n","    avg_loss = total_loss/len(test_loader.dataset)\n","    debug('Epoch %d : Loss %.4f Accuracy %.5f' % (epoch, avg_loss, acc), DEBUG_L1)\n","    debug('All pred dim: %d' % len(all_pred), DEBUG_L2)\n","\n","    is_best = acc > bestacc\n","    if is_best:\n","        bestacc = acc\n","        best_pred = all_pred.numpy()\n","    if args.dictionarysize!=20000:\n","        fname = \"models/advtrain_combined_swap_\" + model_name +str(args.dictionarysize) + \"_\" + str(args.data)\n","    else:\n","        fname = \"models/advtrain_combined_swap_\" + model_name + \"_\" + str(args.data)\n","          \n","    save_checkpoint({'epoch': epoch + 1,\n","              'state_dict': model.state_dict(),\n","              'bestacc': bestacc,\n","              'optimizer' : optimizer.state_dict(),\n","          }, is_best, filename = fname)\n","\n","  return best_pred\n","\n","def test_model(model, data_loader):\n","  \"\"\"\n","    Test a given model on the data provided.\n","\n","    Returns:\n","    all_probs: probabilities returned by the model.\n","    all_preds: class predictions deduced from model probabilities.\n","\n","    Parameters:\n","    model: the model to be tested.\n","    data_loader: a pytorch data loader of the data to be used for testing.\n","  \"\"\"\n","  curr_model = get_model(model, numclass)\n","  curr_model = curr_model.to(device)\n","  model_path = \"models/advtrain_combined_swap_\"+model+\"_0_bestmodel.dat\"\n","  state = torch.load(model_path)\n","\n","  try:\n","      curr_model.load_state_dict(state['state_dict'])\n","  except:\n","      curr_model = torch.nn.DataParallel(model).to(device)\n","      curr_model.load_state_dict(state['state_dict'])\n","      curr_model = curr_model.module\n","\n","  debug(\"Model ready.\", DEBUG_L2)\n","\n","  curr_model.eval()\n","\n","  all_probs = []\n","  all_preds = []\n","  correct = .0\n","  total_loss = 0\n","  for dataid, data in enumerate(data_loader):\n","      debug(\"Test instance #%d...\" % dataid, DEBUG_L2)\n","      try: \n","        inputs, target, idx, raw = data\n","      except:\n","        inputs, target = data\n","      inputs, target = inputs.to(device), target.to(device)\n","      output = curr_model(inputs)\n","      probs_pos = output.data.transpose(0,1)[1] # probability of the positive class\n","      loss = F.nll_loss(output, target)\n","      total_loss += loss.item()\n","      pred = output.data.max(1, keepdim=True)[1]\n","      all_probs.append(probs_pos.cpu())\n","      all_preds.append(pred.cpu())\n","      correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n","\n","  all_probs = torch.cat(all_probs[:-1], dim = 0)\n","  all_probs = torch.cat((all_probs,probs_pos.cpu()), dim = 0)\n","\n","  all_preds = torch.cat(all_preds[:-1], dim = 0)\n","  all_preds = torch.cat((all_preds,pred.cpu()), dim = 0)\n","\n","  return all_probs, all_preds"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-W0jr-NAb3d-","colab_type":"text"},"source":["Train classifiers"]},{"cell_type":"code","metadata":{"id":"PU115uw0N1oM","colab_type":"code","colab":{}},"source":["def train_classifiers(train_loader, test_loader): \n","  \"\"\"\n","    Trains all the models defined in the global object 'models'.\n","\n","    Parameters:\n","    train_loader: the training data.\n","    test_loader: test data used to evaluate the model at every epoch.\n","  \"\"\"\n","\n","  for i in range(len(models)):\n","    debug(\"====== model #%d =========\" % i, DEBUG_L2)\n","    debug(models[i],DEBUG_L2)\n","\n","    if models[i] == \"smallcharrnn\": # minor modification for smallcharrnn\n","        args.charlength = 300\n","\n","    model_instance = get_model(models[i], numclass)\n","\n","    if datatypes[i] == 'word':\n","      train_model(models[i], model_instance, train_loader, test_loader)\n","    elif datatypes[i] == 'char':\n","      train_model(models[i], model_instance, train_loader, test_loader)\n","\n","train_classifiers(trainword_loader, testword_loader)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zJkyPaGOWiaF","colab_type":"text"},"source":["**Below code is in the process of refactoring**"]},{"cell_type":"code","metadata":{"id":"SkINRcfu8kx4","colab_type":"code","colab":{}},"source":["def evaluate_classifiers(data_loader, labels):\n","  fpr_l = []\n","  tpr_l = []\n","  auc_l = []\n","\n","  models = [\"bilstm\",\"simplernn\",\"wordcnn\"]\n","  datatypes = [\"word\",\"word\",\"word\"]\n","  for i in range(len(models)):\n","    debug(\"Loaded data.\", DEBUG_L2)\n","    debug(\"Creating model...\", DEBUG_L2)\n","\n","    probs, preds = test_model(models[i], data_loader)\n","  \n","    f1 = f1_score(labels, preds)\n","\n","    log = open('train_f1.txt','a')\n","    log.write('%d\\t%s\\t%.2f\\n' % (args.data, models[i], 100*f1))\n","    \n","    debug('F1 score %.5f' % f1, DEBUG_L1)\n","    debug(\"Done testing. Computing ROC...\", DEBUG_L2)\n","\n","    fpr, tpr, _ = roc_curve(labels, probs)\n","    area = auc(fpr,tpr)\n","\n","    fpr_l.append(fpr)\n","    tpr_l.append(tpr)\n","    auc_l.append(area)\n","\n","  plot_roc(fpr_l, tpr_l, auc_l);\n","  debug(\"Done. ROC plot generated.\", DEBUG_L2)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lRamXm63jNV7","colab_type":"code","colab":{}},"source":["alltimebest = 0\n","bestfeature = []\n","def recoveradv(rawsequence, index2word, inputs, advwords):\n","    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n ' # not used??\n","    rear_ct = len(rawsequence)\n","    advsequence = rawsequence[:]\n","    try:\n","        for i in range(inputs.size()[0]-1,-1,-1):\n","            wordi = index2word[inputs[i].item()]\n","            rear_ct = rawsequence[:rear_ct].rfind(wordi)\n","                # print(rear_ct)\n","            if inputs[i].item()>=3:\n","                advsequence = advsequence[:rear_ct] + advwords[i] + advsequence[rear_ct + len(wordi):]\n","    except:\n","        print('something went wrong')\n","    return advsequence\n","    \n","def attackword(model_name, model_instance, data_loader, scoring_fn, trans_fn, maxbatch = None):\n","    corrects = .0\n","    total_loss = 0\n","    model_instance.eval()\n","    wordinput = []\n","    tgt = []\n","    adv = []\n","    origsample = []\n","    origsampleidx = []\n","    \n","    for dataid, data in enumerate(data_loader):\n","        debug(dataid, DEBUG_L1)\n","        if maxbatch!=None and dataid >= maxbatch:\n","            break\n","        inputs, target, idx, raw = data\n","        inputs, target = inputs.to(device), target.to(device)\n","        origsample.append(inputs)\n","        origsampleidx.append(idx)\n","        tgt.append(target)\n","        wtmp = []\n","        output = model_instance(inputs)\n","        pred = torch.max(output, 1)[1].view(target.size())\n","        \n","        losses = scoring.scorefunc(scoring_fn)(model_instance, inputs, pred, numclass)\n","        \n","        sorted, indices = torch.sort(losses, dim=1, descending=True)\n","\n","        advinputs = inputs.clone()\n","        \n","        for k in range(inputs.size()[0]):\n","            wtmp.append([])\n","            for i in range(inputs.size()[1]):\n","                if advinputs[k,i].item()>3: # word beyond padding\n","                    wtmp[-1].append(index2word[advinputs[k,i].item()])\n","                else:\n","                    wtmp[-1].append('')\n","        # change important words\n","        debug(\"========= words changed ========\", DEBUG_L2)\n","        for k in range(inputs.size()[0]):\n","            j = 0\n","            t = 0\n","            while j < args.power and t<inputs.size()[1]:\n","                if advinputs[k,indices[k][t]].item()>3:\n","                  word, advinputs[k,indices[k][t]] = transformer.transform(trans_fn)(advinputs[k,indices[k][t]].item(), word_index, index2word, top_words = args.dictionarysize)\n","                  wtmp[k][indices[k][t]] = word\n","                  debug(word, DEBUG_L2)\n","                  j+=1\n","                t+=1\n","        adv.append(advinputs)\n","        \n","        output2 = model_instance(advinputs)\n","        pred2 = torch.max(output2, 1)[1].view(target.size())\n","        \n","        corrects += (pred2 == target).sum().item()\n","        for i in range(len(wtmp)):\n","          if pred[i].item() != pred2[i].item(): \n","            debug(raw[i], DEBUG_L2)\n","            debug(pred[i].item(), DEBUG_L2)\n","            wordinputi = recoveradv(raw[i], index2word, inputs[i], wtmp[i])\n","            debug(wordinputi, DEBUG_L2)\n","            wordinput.append(wordinputi)\n","            debug(pred2[i].item(), DEBUG_L2)\n","\n","    target = torch.cat(tgt)\n","    advinputs = torch.cat(adv)\n","    origsamples = torch.cat(origsample)\n","    origsampleidx = torch.cat(origsampleidx)\n","    acc = corrects/advinputs.size(0)\n","    debug('Accuracy %.5f' % acc, DEBUG_L1)\n","    f = open('attack_log.txt','a')\n","    f.write('%d\\t%d\\t%s\\t%s\\t%s\\t%d\\t%.2f\\n' % (args.data, args.wordlength, model_name, scoring_fn, trans_fn, args.power, 100*acc))\n","    if args.advsamplepath == None:\n","        advsamplepath = 'advsamples/test_%s_%d_%s_%s_%d_%d.dat' % (model_name, args.data, scoring_fn, trans_fn, args.power, args.wordlength)\n","    else:\n","        advsamplepath = args.advsamplepath\n","    torch.save({'original':origsamples, 'sampleid':origsampleidx, 'wordinput':wordinput, 'advinputs':advinputs, 'labels':target}, advsamplepath)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0r3hECl2MUY","colab_type":"code","colab":{}},"source":["# load adv exampels\n","\n","adv_paths = [\"advsamples/bilstm_0_combined_swap_1_500.dat\", \n","         \"advsamples/bilstm_0_combined_homoglyph_1_500.dat\",\n","         \"advsamples/bilstm_0_random_swap_1_500.dat\",\n","         \"advsamples/bilstm_0_random_homoglyph_1_500.dat\"]\n","\n","path = \"advsamples/test_bilstm_0_combined_homoglyph_1_500.dat\"\n","data = torch.load(path)\n","\n","debug(\"Loaded the following data items: %s\" % data.keys(), DEBUG_L2)\n","\n","inputs = data['advinputs'].cpu()\n","labels = data['labels'].cpu()\n","\n","debug(inputs, DEBUG_L2)\n","\n","debug(\"Loaded adv samples of size: %d\" % len(inputs), DEBUG_L2)\n","debug(\"Loaded target data of size: %d\" % len(labels), DEBUG_L2)\n","\n","adv_set = advData(inputs,labels)\n","\n","adv_loader = DataLoader(adv_set, batch_size = args.batchsize, num_workers = 4, shuffle = True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKyjnjoUqba1","colab_type":"code","colab":{}},"source":["class advData(Dataset):\n","    def __init__(self, inputs, outputs, tokenizer = True, length=1014, space = False, backward = -1, getidx = False, rawdata = None):\n","        self.backward = backward\n","        self.length = length\n","        (self.inputs,self.labels) = (inputs,outputs)\n","        # self.labels = torch.LongTensor(self.labels)\n","        # self.inputs = torch.from_numpy(self.inputs).long()\n","        self.getidx = getidx\n","        if rawdata:\n","            self.raw = rawdata\n","    def __len__(self):\n","        return len(self.inputs)\n","    def __getitem__(self,idx):\n","        x = self.inputs[idx]\n","        y = self.labels[idx]\n","        if self.getidx==True:\n","            if self.raw:\n","                return x,y,idx,self.raw[idx]\n","            else:\n","                return x,y,idx\n","        else:\n","            return x,y\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6cZg4gnCJdu","colab_type":"code","colab":{}},"source":["paths = [\"advsamples/bilstm_0_combined_swap_1_500.dat\", \n","         \"advsamples/bilstm_0_combined_homoglyph_1_500.dat\",\n","         \"advsamples/bilstm_0_random_swap_1_500.dat\",\n","         \"advsamples/bilstm_0_random_homoglyph_1_500.dat\"]\n","\n","path = \"advsamples/bilstm_0_random_homoglyph_1_500.dat\"\n","data = torch.load(path)\n","\n","debug(\"Loaded the following data items: %s\" % data.keys(), DEBUG_L2)\n","\n","inputs = data['advinputs'].cpu()\n","labels = data['labels'].cpu()\n","\n","debug(inputs, DEBUG_L2)\n","\n","debug(\"Loaded adv samples of size: %d\" % len(inputs), DEBUG_L2)\n","debug(\"Loaded target data of size: %d\" % len(labels), DEBUG_L2)\n","\n","adv_set = advData(inputs,labels)\n","\n","# adv_loader = DataLoader(adv_set, batch_size = args.batchsize, num_workers = 4, shuffle = True)\n","\n","# augment adversarial set\n","adv_train_set_inputs = trainword_set.inputs.clone()\n","adv_train_set_labels = trainword_set.labels.clone()\n","\n","debug(\"Train samples of size: %d\" % len(adv_train_set_inputs), DEBUG_L2)\n","debug(\"Train target samples of size: %d\" % len(adv_train_set_labels), DEBUG_L2)\n","\n","adv_train_set_inputs = torch.cat((adv_train_set_inputs, adv_set.inputs), dim=0)\n","adv_train_set_labels = torch.cat((adv_train_set_labels, adv_set.labels), dim=0)\n","\n","debug(\"Adv train samples of size: %d\" % len(adv_train_set_inputs), DEBUG_L2)\n","debug(\"Adv train target samples of size: %d\" % len(adv_train_set_labels), DEBUG_L2)\n","\n","adv_train_set = advData(adv_train_set_inputs, adv_train_set_labels)\n","\n","adv_train_loader = DataLoader(adv_train_set, batch_size = args.batchsize, num_workers = 4, shuffle = True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZUiOFF2VR94D","colab_type":"code","colab":{}},"source":[" import copy\n","\n","paths = [\"advsamples/bilstm_0_combined_swap_1_500.dat\", \n","         \"advsamples/bilstm_0_combined_homoglyph_1_500.dat\",\n","         \"advsamples/bilstm_0_random_swap_1_500.dat\",\n","         \"advsamples/bilstm_0_random_homoglyph_1_500.dat\"]\n","\n","path = \"advsamples/bilstm_0_combined_homoglyph_1_500.dat\"\n","data = torch.load(path)\n","\n","debug(\"Loaded the following data items: %s\" % data.keys(), DEBUG_L2)\n","\n","inputs = data['advinputs'].cpu()\n","labels = data['labels'].cpu()\n","\n","debug(inputs, DEBUG_L2)\n","\n","debug(\"Loaded adv samples of size: %d\" % len(inputs), DEBUG_L2)\n","debug(\"Loaded target data of size: %d\" % len(labels), DEBUG_L2)\n","\n","adv_set = advData(inputs,labels)\n","\n","# adv_loader = DataLoader(adv_set, batch_size = args.batchsize, num_workers = 4, shuffle = True)\n","\n","# augment adversarial set\n","adv_train_set_inputs = trainword_set.inputs.clone()\n","adv_train_set_labels = trainword_set.labels.clone()\n","\n","debug(\"Train samples of size: %d\" % len(adv_train_set_inputs), DEBUG_L2)\n","debug(\"Train target samples of size: %d\" % len(adv_train_set_labels), DEBUG_L2)\n","\n","adv_train_set_inputs = torch.cat((adv_train_set_inputs, adv_set.inputs), dim=0)\n","adv_train_set_labels = torch.cat((adv_train_set_labels, adv_set.labels), dim=0)\n","\n","debug(\"Adv train samples of size: %d\" % len(adv_train_set_inputs), DEBUG_L2)\n","debug(\"Adv train target samples of size: %d\" % len(adv_train_set_labels), DEBUG_L2)\n","\n","adv_train_set = advData(adv_train_set_inputs, adv_train_set_labels)\n","\n","adv_train_loader = DataLoader(adv_train_set, batch_size = args.batchsize, num_workers = 4, shuffle = True)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jA7jfqHMcxAQ","colab_type":"code","colab":{}},"source":["# attack models\n","models = [\"bilstm\"]\n","datatypes = [\"word\"]\n","\n","transformers = {\n","    'word': [\"homoglyph\"]\n","}\n","\n","scorings = {\n","    'word': [\"combined\"]\n","}\n","\n","# transformers = {\n","#     'word': [\"swap\",\"flip\",\"f2\",\"insert\",\"remove\",\"r2\",\"homoglyph\"],\n","#     'char': [\"remove\",\"flip\",\"homoglyph\"]\n","# }\n","\n","# scorings = {\n","#     'word': [\"temporal\",\"tail\",\"combined\",\"replaceone\",\"random\",\"ucgrad\",\"grad\"],\n","#     'char': [\"temporal\",\"tail\",\"combined\",\"replaceone\",\"random\",\"grad\"]\n","# }\n","\n","torch.manual_seed(8)\n","torch.cuda.manual_seed(8)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","debug(\"Loading model state...\", DEBUG_L2)\n","\n","for m in range(len(models)):\n","\n","  trans_fns = transformers[datatypes[m]]\n","  scoring_fns = scorings[datatypes[m]]\n","\n","  for t in range(len(trans_fns)):\n","    for s in range(len(scoring_fns)):\n","\n","      model_instance = get_model(models[m], numclass)\n","      model_path = \"models/\"+models[m]+\"_0_bestmodel.dat\"\n","      state = torch.load(model_path)\n","      model_instance = model_instance.to(device)\n","\n","      try:\n","          model_instance.load_state_dict(state['state_dict'])\n","      except:\n","          model_instance = torch.nn.DataParallel(model_instance).to(device)\n","          model_instance.load_state_dict(state['state_dict'])\n","          model_instance = model_instance.module\n","\n","      debug(\"Model state loaded...\", DEBUG_L2)\n","\n","      debug(\"Attacking model...\", DEBUG_L2)\n","      if datatypes[m] == \"char\":\n","        attackchar(models[m], model_instance, advchar_loader, scoring_fns[s], trans_fns[t], maxbatch = args.maxbatches)\n","      elif datatypes[m] == \"word\":\n","        index2word = {}\n","        index2word[0] = '[PADDING]'\n","        index2word[1] = '[START]'\n","        index2word[2] = '[UNKNOWN]'\n","        index2word[3] = ''\n","        if args.dictionarysize==20000:\n","          for i in word_index:\n","            if word_index[i]+3 < args.dictionarysize:\n","                index2word[word_index[i]+3]=i\n","        else:\n","          for i in word_index:\n","            if word_index[i] + 3 < args.dictionarysize:\n","                index2word[word_index[i]+3]=i  \n","        attackword(models[m], model_instance, testword_loader, scoring_fns[s], trans_fns[t], maxbatch = args.maxbatches)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4gd8XKSw4v1v","colab_type":"text"},"source":["# **Stage 1:** Regular detection\n"]},{"cell_type":"code","metadata":{"id":"M07oOyJr6rpv","colab_type":"code","colab":{}},"source":["# code below will generate AUC-ROC curves and F1-scores \n","# for the performance of all classifiers on test data.\n","\n","evaluate_classifiers(testword_loader, labels)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8EUxqwTp7nZ5","colab_type":"text"},"source":["# **Stage 2:** Adversarial attack"]},{"cell_type":"code","metadata":{"id":"0NXRz_NCYVk2","colab_type":"code","colab":{}},"source":["evaluate_classifiers(adv_loader, labels)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V9_Di3zB7uv6","colab_type":"text"},"source":["# **Stage 3:** Retraining on adversarially augmented data"]},{"cell_type":"code","metadata":{"id":"SjoKv_S871A9","colab_type":"code","colab":{}},"source":["train_classifiers(adv_trainword_loader, testword_loader)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l5si7l4X79hu","colab_type":"text"},"source":["# **Stage 4:** Evaluating performance on adversarial sample after retraining "]},{"cell_type":"code","metadata":{"id":"BzYDyI0H8Lkl","colab_type":"code","colab":{}},"source":["evaluate_classifiers(adv_loader, labels)"],"execution_count":0,"outputs":[]}]}